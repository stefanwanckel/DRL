# WITH SAC

widowx_reacher-v2:
  n_timesteps: 500000
  normalize: true
  policy: 'MlpPolicy'
  model_class: 'td3'
  n_sampled_goal: 4
  goal_selection_strategy: "future"
  online_sampling: True
  max_episode_length: 100
  # policy_kwargs: "dict(net_arch=[256, 256, 256])"
  # env_wrapper:
  #   - utils.wrappers.HistoryWrapperObsDict:
  #       horizon: 2
  #   - utils.wrappers.TimeFeatureWrapper

widowx_reacher-v3:
  n_timesteps: 30000
  normalize: true
  policy: 'MlpPolicy'
  model_class: 'td3'
  n_sampled_goal: 4
  goal_selection_strategy: "future"
  online_sampling: True
  max_episode_length: 300
  # policy_kwargs: "dict(net_arch=[256, 256, 256])"
  # env_wrapper:
  #   - utils.wrappers.HistoryWrapperObsDict:
  #       horizon: 2
  #   - utils.wrappers.TimeFeatureWrapper

widowx_reacher-v4:
  n_timesteps: 500000
  normalize: true
  policy: 'MlpPolicy'
  model_class: 'sac'
  n_sampled_goal: 4
  goal_selection_strategy: "future"
  online_sampling: True
  max_episode_length: 100
  # policy_kwargs: "dict(net_arch=[256, 256, 256])"
  # env_wrapper:
  #   - utils.wrappers.HistoryWrapperObsDict:
  #       horizon: 2
  #   - utils.wrappers.TimeFeatureWrapper


##############
# WITH TD3

# widowx_reacher-v2:
#   n_timesteps: 500000
#   normalize: true
#   policy: 'MlpPolicy'
#   model_class: 'td3'
#   n_sampled_goal: 4
#   goal_selection_strategy: "future"
#   online_sampling: True
#   max_episode_length: 100
#   policy_kwargs: "dict(net_arch=[256, 256, 256])"
#   env_wrapper:
#     - utils.wrappers.HistoryWrapperObsDict:
#         horizon: 2
#     - utils.wrappers.TimeFeatureWrapper



# widowx_reacher-v4:
#   n_timesteps: 500000
#   normalize: true
#   policy: 'MlpPolicy'
#   model_class: 'td3'
#   n_sampled_goal: 4
#   goal_selection_strategy: "future"
#   online_sampling: True
#   max_episode_length: 100
#   policy_kwargs: "dict(net_arch=[256, 256, 256])"
#   env_wrapper:
#     - utils.wrappers.HistoryWrapperObsDict:
#         horizon: 2
#     - utils.wrappers.TimeFeatureWrapper
